{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rc(\"figure\", autolayout=True)\n",
    "plt.rc(\n",
    "    \"axes\",\n",
    "    labelweight=\"bold\",\n",
    "    labelsize=\"large\",\n",
    "    titleweight=\"bold\",\n",
    "    titlesize=14,\n",
    "    titlepad=10,\n",
    ")\n",
    "\n",
    "# accidents = pd.read_csv(\"../input/fe-course-data/accidents.csv\")\n",
    "# autos = pd.read_csv(\"../input/fe-course-data/autos.csv\")\n",
    "concrete = pd.read_csv(\"data/concrete.csv\")\n",
    "# customer = pd.read_csv(\"../input/fe-course-data/customer.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips on Discovering New Features\n",
    "# Understand the features. Refer to your dataset's data documentation, if available.\n",
    "\n",
    "# Research the problem domain to acquire domain knowledge. If your problem is predicting house prices, \n",
    "# do some research on real-estate for instance. Wikipedia can be a good starting point, but books and \n",
    "# journal articles will often have the best information.\n",
    "\n",
    "# Study previous work. Solution write-ups from past Kaggle competitions are a great resource.\n",
    "\n",
    "# Use data visualization. Visualization can reveal pathologies in the distribution of a feature or \n",
    "# complicated relationships that could be simplified. Be sure to visualize your dataset as you work \n",
    "# through the feature engineering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autos[\"stroke_ratio\"] = autos.stroke / autos.bore\n",
    "\n",
    "# autos[[\"stroke\", \"bore\", \"stroke_ratio\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autos[\"displacement\"] = (\n",
    "#     np.pi * ((0.5 * autos.bore) ** 2) * autos.stroke * autos.num_of_cylinders\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If the feature has 0.0 values, use np.log1p (log(1+x)) instead of np.log\n",
    "# accidents[\"LogWindSpeed\"] = accidents.WindSpeed.apply(np.log1p)\n",
    "\n",
    "# # Plot a comparison\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "# sns.kdeplot(accidents.WindSpeed, shade=True, ax=axs[0])\n",
    "# sns.kdeplot(accidents.LogWindSpeed, shade=True, ax=axs[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# components = [ \"Cement\", \"BlastFurnaceSlag\", \"FlyAsh\", \"Water\",\n",
    "#                \"Superplasticizer\", \"CoarseAggregate\", \"FineAggregate\"]\n",
    "# concrete[\"Components\"] = concrete[components].gt(0).sum(axis=1)\n",
    "\n",
    "# concrete[components + [\"Components\"]].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tips on Creating Features\n",
    "\n",
    "# It's good to keep in mind your model's own strengths and weaknesses when creating features. Here are some guidelines:\n",
    "\n",
    "# Linear models learn sums and differences naturally, but can't learn anything more complex.\n",
    "\n",
    "# Ratios seem to be difficult for most models to learn. Ratio combinations often lead to some easy performance gains.\n",
    "\n",
    "# Linear models and neural nets generally do better with normalized features. Neural nets especially need features scaled \n",
    "# to values not too far from 0. Tree-based models (like random forests and XGBoost) can sometimes benefit from normalization, but usually much less so.\n",
    "\n",
    "# Tree models can learn to approximate almost any combination of features, but when a combination is especially important\n",
    "#  they can still benefit from having it explicitly created, especially when data is limited.\n",
    "\n",
    "# Counts are especially helpful for tree models, since these models don't have a natural way of aggregating information across many features at once."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a245dac45e51386e3067f7bf875d7c30851316ec82b938f00e050e0cf1d2e3a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
