{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An activation function is simply some function we apply to each of a layer's outputs (its activations).\n",
    "# The most common is the rectifier function  max(0,x) .\n",
    "\n",
    "# When we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. \n",
    "# (For this reason, it's common to call the rectifier function the \"ReLU function\".) \n",
    "# Applying a ReLU activation to a linear unit means the output becomes max(0, w * x + b),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    # the hidden ReLU layers\n",
    "    layers.Dense(units=4, activation='relu', input_shape=[2]),\n",
    "    layers.Dense(units=3, activation='relu'),\n",
    "    # the linear output layer \n",
    "    layers.Dense(units=1),\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a245dac45e51386e3067f7bf875d7c30851316ec82b938f00e050e0cf1d2e3a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
